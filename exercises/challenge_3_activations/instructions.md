# Challenge 3: The Activation Lab ðŸ§ª

**Objective:** Implement and visualize custom activation functions.

## The Mission
Implement `Sigmoid` and `Leaky ReLU` from scratch and visualize them.

## Tasks
1.  **Implement Sigmoid**:
    *   Formula: `1 / (1 + e^-x)`
    *   Range: (0, 1)
2.  **Implement Leaky ReLU**:
    *   Formula: `x` if `x > 0`, else `0.01 * x`
    *   Why? Fixes "dead neurons" by allowing a small gradient for negatives.
3.  **Run the script**: It will plot your functions.

## Expected Output
A plot showing both activation functions correctly.
